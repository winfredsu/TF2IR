IR def problems:
1. Should the uniqueue be a ordered list?
2. 'operation' field naming conventions?
3. What is 'load_bias'?
4. How to recognize the w/b raw param file name?
5. How to add 'test tensors'?
6. Input tensor and endpoints?
7. 'add' and 'pl' in add op?
8. Reshape and Concat op definition. 

{
    "Layer_0_Conv": {
        "name": "Layer_0_Conv",
        "operation": "conv",
        "output_shift": 9,
        "bias_shift": 12,
        "load_bias": false,
        "input_channel_num": 3,
        "output_channel_num": 64,
        "input_size": {
            "height": 32,
            "width": 32
        },
        "padding": {
            "top": 1,
            "bottom": 1,
            "left": 1,
            "right": 1
        },
        "stride": {
            "height": 1,
            "width": 1
        },
        "kernel_size": {
            "height": 3,
            "width": 3
        },
        "output_size": {
            "height": 32,
            "width": 32
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "weight_dtype": "int8",
        "bias_dtype": "float",
        "previous_layer": [
            "input"
        ],
        "next_layer": [
            "Layer_1_BatchNormalization"
        ]
    },
    "Layer_1_BatchNormalization": {
        "name": "Layer_1_BatchNormalization",
        "operation": "batchnorm",
        "input_size": {
            "height": 32,
            "width": 32
        },
        "input_channel_num": 64,
        "input_dtype": "int8",
        "input_div_shift": 3,
        "mean_dtype": "float",
        "beta_dtype": "float",
        "var_dtype": "float",
        "scale_dtype": "float",
        "output_dtype": "int8",
        "output_scale": 5,
        "epsilon": 9.999999747378752e-06,
        "previous_layer": [
            "Layer_0_Conv"
        ],
        "next_layer": [
            "Layer_2_Relu"
        ]
    },
    "Layer_2_Relu": {
        "name": "Layer_2_Relu",
        "operation": "relu",
        "input_channel_num": 64,
        "input_size": {
            "height": 32,
            "width": 32
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "previous_layer": [
            "Layer_1_BatchNormalization"
        ],
        "next_layer": [
            "Layer_3_Conv",
            "Layer_8_Add"
        ]
    },
    "Layer_3_Conv": {
        "name": "Layer_3_Conv",
        "operation": "conv",
        "output_shift": 8,
        "bias_shift": 12,
        "load_bias": false,
        "input_channel_num": 64,
        "output_channel_num": 64,
        "input_size": {
            "height": 32,
            "width": 32
        },
        "padding": {
            "top": 1,
            "bottom": 1,
            "left": 1,
            "right": 1
        },
        "stride": {
            "height": 1,
            "width": 1
        },
        "kernel_size": {
            "height": 3,
            "width": 3
        },
        "output_size": {
            "height": 32,
            "width": 32
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "weight_dtype": "int8",
        "bias_dtype": "float",
        "previous_layer": [
            "Layer_2_Relu"
        ],
        "next_layer": [
            "Layer_4_BatchNormalization"
        ]
    },
    "Layer_4_BatchNormalization": {
        "name": "Layer_4_BatchNormalization",
        "operation": "batchnorm",
        "input_size": {
            "height": 32,
            "width": 32
        },
        "input_channel_num": 64,
        "input_dtype": "int8",
        "input_div_shift": 4,
        "mean_dtype": "float",
        "beta_dtype": "float",
        "var_dtype": "float",
        "scale_dtype": "float",
        "output_dtype": "int8",
        "output_scale": 4,
        "epsilon": 9.999999747378752e-06,
        "previous_layer": [
            "Layer_3_Conv"
        ],
        "next_layer": [
            "Layer_5_Relu"
        ]
    },
    "Layer_5_Relu": {
        "name": "Layer_5_Relu",
        "operation": "relu",
        "input_channel_num": 64,
        "input_size": {
            "height": 32,
            "width": 32
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "previous_layer": [
            "Layer_4_BatchNormalization"
        ],
        "next_layer": [
            "Layer_6_Conv"
        ]
    },
    "Layer_6_Conv": {
        "name": "Layer_6_Conv",
        "operation": "conv",
        "output_shift": 6,
        "bias_shift": 11,
        "load_bias": false,
        "input_channel_num": 64,
        "output_channel_num": 64,
        "input_size": {
            "height": 32,
            "width": 32
        },
        "padding": {
            "top": 1,
            "bottom": 1,
            "left": 1,
            "right": 1
        },
        "stride": {
            "height": 1,
            "width": 1
        },
        "kernel_size": {
            "height": 3,
            "width": 3
        },
        "output_size": {
            "height": 32,
            "width": 32
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "weight_dtype": "int8",
        "bias_dtype": "float",
        "previous_layer": [
            "Layer_5_Relu"
        ],
        "next_layer": [
            "Layer_7_BatchNormalization"
        ]
    },
    "Layer_7_BatchNormalization": {
        "name": "Layer_7_BatchNormalization",
        "operation": "batchnorm",
        "input_size": {
            "height": 32,
            "width": 32
        },
        "input_channel_num": 64,
        "input_dtype": "int8",
        "input_div_shift": 5,
        "mean_dtype": "float",
        "beta_dtype": "float",
        "var_dtype": "float",
        "scale_dtype": "float",
        "output_dtype": "int8",
        "output_scale": 5,
        "epsilon": 9.999999747378752e-06,
        "previous_layer": [
            "Layer_6_Conv"
        ],
        "next_layer": [
            "Layer_8_Add"
        ]
    },
    "Layer_8_Add": {
        "name": "Layer_8_Add",
        "operation": "add",
        "size": 65536,
        "dtype": "int8",
        "pl_shift_bit": 5,
        "add_shift_bit": 5,
        "output_shift_bit": -1,
        "previous_layer": [
            "Layer_7_BatchNormalization",
            "Layer_2_Relu"
        ],
        "pl_name": "Layer_7_BatchNormalization",
        "add_name": "Layer_2_Relu",
        "next_layer": [
            "Layer_9_Relu"
        ]
    },
    "Layer_9_Relu": {
        "name": "Layer_9_Relu",
        "operation": "relu",
        "input_channel_num": 64,
        "input_size": {
            "height": 32,
            "width": 32
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "previous_layer": [
            "Layer_8_Add"
        ],
        "next_layer": [
            "Layer_10_Conv",
            "Layer_15_Add"
        ]
    },
    "Layer_10_Conv": {
        "name": "Layer_10_Conv",
        "operation": "conv",
        "output_shift": 8,
        "bias_shift": 11,
        "load_bias": false,
        "input_channel_num": 64,
        "output_channel_num": 64,
        "input_size": {
            "height": 32,
            "width": 32
        },
        "padding": {
            "top": 1,
            "bottom": 1,
            "left": 1,
            "right": 1
        },
        "stride": {
            "height": 1,
            "width": 1
        },
        "kernel_size": {
            "height": 3,
            "width": 3
        },
        "output_size": {
            "height": 32,
            "width": 32
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "weight_dtype": "int8",
        "bias_dtype": "float",
        "previous_layer": [
            "Layer_9_Relu"
        ],
        "next_layer": [
            "Layer_11_BatchNormalization"
        ]
    },
    "Layer_11_BatchNormalization": {
        "name": "Layer_11_BatchNormalization",
        "operation": "batchnorm",
        "input_size": {
            "height": 32,
            "width": 32
        },
        "input_channel_num": 64,
        "input_dtype": "int8",
        "input_div_shift": 3,
        "mean_dtype": "float",
        "beta_dtype": "float",
        "var_dtype": "float",
        "scale_dtype": "float",
        "output_dtype": "int8",
        "output_scale": 5,
        "epsilon": 9.999999747378752e-06,
        "previous_layer": [
            "Layer_10_Conv"
        ],
        "next_layer": [
            "Layer_12_Relu"
        ]
    },
    "Layer_12_Relu": {
        "name": "Layer_12_Relu",
        "operation": "relu",
        "input_channel_num": 64,
        "input_size": {
            "height": 32,
            "width": 32
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "previous_layer": [
            "Layer_11_BatchNormalization"
        ],
        "next_layer": [
            "Layer_13_Conv"
        ]
    },
    "Layer_13_Conv": {
        "name": "Layer_13_Conv",
        "operation": "conv",
        "output_shift": 8,
        "bias_shift": 13,
        "load_bias": false,
        "input_channel_num": 64,
        "output_channel_num": 64,
        "input_size": {
            "height": 32,
            "width": 32
        },
        "padding": {
            "top": 1,
            "bottom": 1,
            "left": 1,
            "right": 1
        },
        "stride": {
            "height": 1,
            "width": 1
        },
        "kernel_size": {
            "height": 3,
            "width": 3
        },
        "output_size": {
            "height": 32,
            "width": 32
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "weight_dtype": "int8",
        "bias_dtype": "float",
        "previous_layer": [
            "Layer_12_Relu"
        ],
        "next_layer": [
            "Layer_14_BatchNormalization"
        ]
    },
    "Layer_14_BatchNormalization": {
        "name": "Layer_14_BatchNormalization",
        "operation": "batchnorm",
        "input_size": {
            "height": 32,
            "width": 32
        },
        "input_channel_num": 64,
        "input_dtype": "int8",
        "input_div_shift": 5,
        "mean_dtype": "float",
        "beta_dtype": "float",
        "var_dtype": "float",
        "scale_dtype": "float",
        "output_dtype": "int8",
        "output_scale": 5,
        "epsilon": 9.999999747378752e-06,
        "previous_layer": [
            "Layer_13_Conv"
        ],
        "next_layer": [
            "Layer_15_Add"
        ]
    },
    "Layer_15_Add": {
        "name": "Layer_15_Add",
        "operation": "add",
        "size": 65536,
        "dtype": "int8",
        "pl_shift_bit": 5,
        "add_shift_bit": 4,
        "output_shift_bit": 1,
        "previous_layer": [
            "Layer_14_BatchNormalization",
            "Layer_9_Relu"
        ],
        "pl_name": "Layer_14_BatchNormalization",
        "add_name": "Layer_9_Relu",
        "next_layer": [
            "Layer_16_Relu"
        ]
    },
    "Layer_16_Relu": {
        "name": "Layer_16_Relu",
        "operation": "relu",
        "input_channel_num": 64,
        "input_size": {
            "height": 32,
            "width": 32
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "previous_layer": [
            "Layer_15_Add"
        ],
        "next_layer": [
            "Layer_17_Conv",
            "Layer_22_Conv"
        ]
    },
    "Layer_17_Conv": {
        "name": "Layer_17_Conv",
        "operation": "conv",
        "output_shift": 10,
        "bias_shift": 14,
        "load_bias": false,
        "input_channel_num": 64,
        "output_channel_num": 128,
        "input_size": {
            "height": 32,
            "width": 32
        },
        "padding": {
            "top": 1,
            "bottom": 1,
            "left": 1,
            "right": 1
        },
        "stride": {
            "height": 2,
            "width": 2
        },
        "kernel_size": {
            "height": 3,
            "width": 3
        },
        "output_size": {
            "height": 16,
            "width": 16
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "weight_dtype": "int8",
        "bias_dtype": "float",
        "previous_layer": [
            "Layer_16_Relu"
        ],
        "next_layer": [
            "Layer_18_BatchNormalization"
        ]
    },
    "Layer_18_BatchNormalization": {
        "name": "Layer_18_BatchNormalization",
        "operation": "batchnorm",
        "input_size": {
            "height": 16,
            "width": 16
        },
        "input_channel_num": 128,
        "input_dtype": "int8",
        "input_div_shift": 4,
        "mean_dtype": "float",
        "beta_dtype": "float",
        "var_dtype": "float",
        "scale_dtype": "float",
        "output_dtype": "int8",
        "output_scale": 6,
        "epsilon": 9.999999747378752e-06,
        "previous_layer": [
            "Layer_17_Conv"
        ],
        "next_layer": [
            "Layer_19_Relu"
        ]
    },
    "Layer_19_Relu": {
        "name": "Layer_19_Relu",
        "operation": "relu",
        "input_channel_num": 128,
        "input_size": {
            "height": 16,
            "width": 16
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "previous_layer": [
            "Layer_18_BatchNormalization"
        ],
        "next_layer": [
            "Layer_20_Conv"
        ]
    },
    "Layer_20_Conv": {
        "name": "Layer_20_Conv",
        "operation": "conv",
        "output_shift": 9,
        "bias_shift": 14,
        "load_bias": false,
        "input_channel_num": 128,
        "output_channel_num": 128,
        "input_size": {
            "height": 16,
            "width": 16
        },
        "padding": {
            "top": 1,
            "bottom": 1,
            "left": 1,
            "right": 1
        },
        "stride": {
            "height": 1,
            "width": 1
        },
        "kernel_size": {
            "height": 3,
            "width": 3
        },
        "output_size": {
            "height": 16,
            "width": 16
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "weight_dtype": "int8",
        "bias_dtype": "float",
        "previous_layer": [
            "Layer_19_Relu"
        ],
        "next_layer": [
            "Layer_21_BatchNormalization"
        ]
    },
    "Layer_21_BatchNormalization": {
        "name": "Layer_21_BatchNormalization",
        "operation": "batchnorm",
        "input_size": {
            "height": 16,
            "width": 16
        },
        "input_channel_num": 128,
        "input_dtype": "int8",
        "input_div_shift": 5,
        "mean_dtype": "float",
        "beta_dtype": "float",
        "var_dtype": "float",
        "scale_dtype": "float",
        "output_dtype": "int8",
        "output_scale": 5,
        "epsilon": 9.999999747378752e-06,
        "previous_layer": [
            "Layer_20_Conv"
        ],
        "next_layer": [
            "Layer_24_Add"
        ]
    },
    "Layer_22_Conv": {
        "name": "Layer_22_Conv",
        "operation": "conv",
        "output_shift": 6,
        "bias_shift": 12,
        "load_bias": false,
        "input_channel_num": 64,
        "output_channel_num": 128,
        "input_size": {
            "height": 32,
            "width": 32
        },
        "padding": {
            "top": 0,
            "bottom": 0,
            "left": 0,
            "right": 0
        },
        "stride": {
            "height": 2,
            "width": 2
        },
        "kernel_size": {
            "height": 1,
            "width": 1
        },
        "output_size": {
            "height": 16,
            "width": 16
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "weight_dtype": "int8",
        "bias_dtype": "float",
        "previous_layer": [
            "Layer_16_Relu"
        ],
        "next_layer": [
            "Layer_23_BatchNormalization"
        ]
    },
    "Layer_23_BatchNormalization": {
        "name": "Layer_23_BatchNormalization",
        "operation": "batchnorm",
        "input_size": {
            "height": 16,
            "width": 16
        },
        "input_channel_num": 128,
        "input_dtype": "int8",
        "input_div_shift": 6,
        "mean_dtype": "float",
        "beta_dtype": "float",
        "var_dtype": "float",
        "scale_dtype": "float",
        "output_dtype": "int8",
        "output_scale": 6,
        "epsilon": 9.999999747378752e-06,
        "previous_layer": [
            "Layer_22_Conv"
        ],
        "next_layer": [
            "Layer_24_Add"
        ]
    },
    "Layer_24_Add": {
        "name": "Layer_24_Add",
        "operation": "add",
        "size": 32768,
        "dtype": "int8",
        "pl_shift_bit": 5,
        "add_shift_bit": 6,
        "output_shift_bit": 0,
        "previous_layer": [
            "Layer_21_BatchNormalization",
            "Layer_23_BatchNormalization"
        ],
        "pl_name": "Layer_21_BatchNormalization",
        "add_name": "Layer_23_BatchNormalization",
        "next_layer": [
            "Layer_25_Relu"
        ]
    },
    "Layer_25_Relu": {
        "name": "Layer_25_Relu",
        "operation": "relu",
        "input_channel_num": 128,
        "input_size": {
            "height": 16,
            "width": 16
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "previous_layer": [
            "Layer_24_Add"
        ],
        "next_layer": [
            "Layer_26_Conv",
            "Layer_31_Add"
        ]
    },
    "Layer_26_Conv": {
        "name": "Layer_26_Conv",
        "operation": "conv",
        "output_shift": 9,
        "bias_shift": 14,
        "load_bias": false,
        "input_channel_num": 128,
        "output_channel_num": 128,
        "input_size": {
            "height": 16,
            "width": 16
        },
        "padding": {
            "top": 1,
            "bottom": 1,
            "left": 1,
            "right": 1
        },
        "stride": {
            "height": 1,
            "width": 1
        },
        "kernel_size": {
            "height": 3,
            "width": 3
        },
        "output_size": {
            "height": 16,
            "width": 16
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "weight_dtype": "int8",
        "bias_dtype": "float",
        "previous_layer": [
            "Layer_25_Relu"
        ],
        "next_layer": [
            "Layer_27_BatchNormalization"
        ]
    },
    "Layer_27_BatchNormalization": {
        "name": "Layer_27_BatchNormalization",
        "operation": "batchnorm",
        "input_size": {
            "height": 16,
            "width": 16
        },
        "input_channel_num": 128,
        "input_dtype": "int8",
        "input_div_shift": 5,
        "mean_dtype": "float",
        "beta_dtype": "float",
        "var_dtype": "float",
        "scale_dtype": "float",
        "output_dtype": "int8",
        "output_scale": 6,
        "epsilon": 9.999999747378752e-06,
        "previous_layer": [
            "Layer_26_Conv"
        ],
        "next_layer": [
            "Layer_28_Relu"
        ]
    },
    "Layer_28_Relu": {
        "name": "Layer_28_Relu",
        "operation": "relu",
        "input_channel_num": 128,
        "input_size": {
            "height": 16,
            "width": 16
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "previous_layer": [
            "Layer_27_BatchNormalization"
        ],
        "next_layer": [
            "Layer_29_Conv"
        ]
    },
    "Layer_29_Conv": {
        "name": "Layer_29_Conv",
        "operation": "conv",
        "output_shift": 9,
        "bias_shift": 15,
        "load_bias": false,
        "input_channel_num": 128,
        "output_channel_num": 128,
        "input_size": {
            "height": 16,
            "width": 16
        },
        "padding": {
            "top": 1,
            "bottom": 1,
            "left": 1,
            "right": 1
        },
        "stride": {
            "height": 1,
            "width": 1
        },
        "kernel_size": {
            "height": 3,
            "width": 3
        },
        "output_size": {
            "height": 16,
            "width": 16
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "weight_dtype": "int8",
        "bias_dtype": "float",
        "previous_layer": [
            "Layer_28_Relu"
        ],
        "next_layer": [
            "Layer_30_BatchNormalization"
        ]
    },
    "Layer_30_BatchNormalization": {
        "name": "Layer_30_BatchNormalization",
        "operation": "batchnorm",
        "input_size": {
            "height": 16,
            "width": 16
        },
        "input_channel_num": 128,
        "input_dtype": "int8",
        "input_div_shift": 6,
        "mean_dtype": "float",
        "beta_dtype": "float",
        "var_dtype": "float",
        "scale_dtype": "float",
        "output_dtype": "int8",
        "output_scale": 6,
        "epsilon": 9.999999747378752e-06,
        "previous_layer": [
            "Layer_29_Conv"
        ],
        "next_layer": [
            "Layer_31_Add"
        ]
    },
    "Layer_31_Add": {
        "name": "Layer_31_Add",
        "operation": "add",
        "size": 32768,
        "dtype": "int8",
        "pl_shift_bit": 6,
        "add_shift_bit": 5,
        "output_shift_bit": 0,
        "previous_layer": [
            "Layer_30_BatchNormalization",
            "Layer_25_Relu"
        ],
        "pl_name": "Layer_30_BatchNormalization",
        "add_name": "Layer_25_Relu",
        "next_layer": [
            "Layer_32_Relu"
        ]
    },
    "Layer_32_Relu": {
        "name": "Layer_32_Relu",
        "operation": "relu",
        "input_channel_num": 128,
        "input_size": {
            "height": 16,
            "width": 16
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "previous_layer": [
            "Layer_31_Add"
        ],
        "next_layer": [
            "Layer_33_Conv",
            "Layer_38_Conv"
        ]
    },
    "Layer_33_Conv": {
        "name": "Layer_33_Conv",
        "operation": "conv",
        "output_shift": 9,
        "bias_shift": 14,
        "load_bias": false,
        "input_channel_num": 128,
        "output_channel_num": 256,
        "input_size": {
            "height": 16,
            "width": 16
        },
        "padding": {
            "top": 1,
            "bottom": 1,
            "left": 1,
            "right": 1
        },
        "stride": {
            "height": 2,
            "width": 2
        },
        "kernel_size": {
            "height": 3,
            "width": 3
        },
        "output_size": {
            "height": 8,
            "width": 8
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "weight_dtype": "int8",
        "bias_dtype": "float",
        "previous_layer": [
            "Layer_32_Relu"
        ],
        "next_layer": [
            "Layer_34_BatchNormalization"
        ]
    },
    "Layer_34_BatchNormalization": {
        "name": "Layer_34_BatchNormalization",
        "operation": "batchnorm",
        "input_size": {
            "height": 8,
            "width": 8
        },
        "input_channel_num": 256,
        "input_dtype": "int8",
        "input_div_shift": 5,
        "mean_dtype": "float",
        "beta_dtype": "float",
        "var_dtype": "float",
        "scale_dtype": "float",
        "output_dtype": "int8",
        "output_scale": 6,
        "epsilon": 9.999999747378752e-06,
        "previous_layer": [
            "Layer_33_Conv"
        ],
        "next_layer": [
            "Layer_35_Relu"
        ]
    },
    "Layer_35_Relu": {
        "name": "Layer_35_Relu",
        "operation": "relu",
        "input_channel_num": 256,
        "input_size": {
            "height": 8,
            "width": 8
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "previous_layer": [
            "Layer_34_BatchNormalization"
        ],
        "next_layer": [
            "Layer_36_Conv"
        ]
    },
    "Layer_36_Conv": {
        "name": "Layer_36_Conv",
        "operation": "conv",
        "output_shift": 9,
        "bias_shift": 15,
        "load_bias": false,
        "input_channel_num": 256,
        "output_channel_num": 256,
        "input_size": {
            "height": 8,
            "width": 8
        },
        "padding": {
            "top": 1,
            "bottom": 1,
            "left": 1,
            "right": 1
        },
        "stride": {
            "height": 1,
            "width": 1
        },
        "kernel_size": {
            "height": 3,
            "width": 3
        },
        "output_size": {
            "height": 8,
            "width": 8
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "weight_dtype": "int8",
        "bias_dtype": "float",
        "previous_layer": [
            "Layer_35_Relu"
        ],
        "next_layer": [
            "Layer_37_BatchNormalization"
        ]
    },
    "Layer_37_BatchNormalization": {
        "name": "Layer_37_BatchNormalization",
        "operation": "batchnorm",
        "input_size": {
            "height": 8,
            "width": 8
        },
        "input_channel_num": 256,
        "input_dtype": "int8",
        "input_div_shift": 6,
        "mean_dtype": "float",
        "beta_dtype": "float",
        "var_dtype": "float",
        "scale_dtype": "float",
        "output_dtype": "int8",
        "output_scale": 6,
        "epsilon": 9.999999747378752e-06,
        "previous_layer": [
            "Layer_36_Conv"
        ],
        "next_layer": [
            "Layer_40_Add"
        ]
    },
    "Layer_38_Conv": {
        "name": "Layer_38_Conv",
        "operation": "conv",
        "output_shift": 7,
        "bias_shift": 13,
        "load_bias": false,
        "input_channel_num": 128,
        "output_channel_num": 256,
        "input_size": {
            "height": 16,
            "width": 16
        },
        "padding": {
            "top": 0,
            "bottom": 0,
            "left": 0,
            "right": 0
        },
        "stride": {
            "height": 2,
            "width": 2
        },
        "kernel_size": {
            "height": 1,
            "width": 1
        },
        "output_size": {
            "height": 8,
            "width": 8
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "weight_dtype": "int8",
        "bias_dtype": "float",
        "previous_layer": [
            "Layer_32_Relu"
        ],
        "next_layer": [
            "Layer_39_BatchNormalization"
        ]
    },
    "Layer_39_BatchNormalization": {
        "name": "Layer_39_BatchNormalization",
        "operation": "batchnorm",
        "input_size": {
            "height": 8,
            "width": 8
        },
        "input_channel_num": 256,
        "input_dtype": "int8",
        "input_div_shift": 6,
        "mean_dtype": "float",
        "beta_dtype": "float",
        "var_dtype": "float",
        "scale_dtype": "float",
        "output_dtype": "int8",
        "output_scale": 6,
        "epsilon": 9.999999747378752e-06,
        "previous_layer": [
            "Layer_38_Conv"
        ],
        "next_layer": [
            "Layer_40_Add"
        ]
    },
    "Layer_40_Add": {
        "name": "Layer_40_Add",
        "operation": "add",
        "size": 16384,
        "dtype": "int8",
        "pl_shift_bit": 6,
        "add_shift_bit": 6,
        "output_shift_bit": 0,
        "previous_layer": [
            "Layer_37_BatchNormalization",
            "Layer_39_BatchNormalization"
        ],
        "pl_name": "Layer_37_BatchNormalization",
        "add_name": "Layer_39_BatchNormalization",
        "next_layer": [
            "Layer_41_Relu"
        ]
    },
    "Layer_41_Relu": {
        "name": "Layer_41_Relu",
        "operation": "relu",
        "input_channel_num": 256,
        "input_size": {
            "height": 8,
            "width": 8
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "previous_layer": [
            "Layer_40_Add"
        ],
        "next_layer": [
            "Layer_42_Conv",
            "Layer_47_Add"
        ]
    },
    "Layer_42_Conv": {
        "name": "Layer_42_Conv",
        "operation": "conv",
        "output_shift": 9,
        "bias_shift": 15,
        "load_bias": false,
        "input_channel_num": 256,
        "output_channel_num": 256,
        "input_size": {
            "height": 8,
            "width": 8
        },
        "padding": {
            "top": 1,
            "bottom": 1,
            "left": 1,
            "right": 1
        },
        "stride": {
            "height": 1,
            "width": 1
        },
        "kernel_size": {
            "height": 3,
            "width": 3
        },
        "output_size": {
            "height": 8,
            "width": 8
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "weight_dtype": "int8",
        "bias_dtype": "float",
        "previous_layer": [
            "Layer_41_Relu"
        ],
        "next_layer": [
            "Layer_43_BatchNormalization"
        ]
    },
    "Layer_43_BatchNormalization": {
        "name": "Layer_43_BatchNormalization",
        "operation": "batchnorm",
        "input_size": {
            "height": 8,
            "width": 8
        },
        "input_channel_num": 256,
        "input_dtype": "int8",
        "input_div_shift": 6,
        "mean_dtype": "float",
        "beta_dtype": "float",
        "var_dtype": "float",
        "scale_dtype": "float",
        "output_dtype": "int8",
        "output_scale": 6,
        "epsilon": 9.999999747378752e-06,
        "previous_layer": [
            "Layer_42_Conv"
        ],
        "next_layer": [
            "Layer_44_Relu"
        ]
    },
    "Layer_44_Relu": {
        "name": "Layer_44_Relu",
        "operation": "relu",
        "input_channel_num": 256,
        "input_size": {
            "height": 8,
            "width": 8
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "previous_layer": [
            "Layer_43_BatchNormalization"
        ],
        "next_layer": [
            "Layer_45_Conv"
        ]
    },
    "Layer_45_Conv": {
        "name": "Layer_45_Conv",
        "operation": "conv",
        "output_shift": 8,
        "bias_shift": 15,
        "load_bias": false,
        "input_channel_num": 256,
        "output_channel_num": 256,
        "input_size": {
            "height": 8,
            "width": 8
        },
        "padding": {
            "top": 1,
            "bottom": 1,
            "left": 1,
            "right": 1
        },
        "stride": {
            "height": 1,
            "width": 1
        },
        "kernel_size": {
            "height": 3,
            "width": 3
        },
        "output_size": {
            "height": 8,
            "width": 8
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "weight_dtype": "int8",
        "bias_dtype": "float",
        "previous_layer": [
            "Layer_44_Relu"
        ],
        "next_layer": [
            "Layer_46_BatchNormalization"
        ]
    },
    "Layer_46_BatchNormalization": {
        "name": "Layer_46_BatchNormalization",
        "operation": "batchnorm",
        "input_size": {
            "height": 8,
            "width": 8
        },
        "input_channel_num": 256,
        "input_dtype": "int8",
        "input_div_shift": 7,
        "mean_dtype": "float",
        "beta_dtype": "float",
        "var_dtype": "float",
        "scale_dtype": "float",
        "output_dtype": "int8",
        "output_scale": 6,
        "epsilon": 9.999999747378752e-06,
        "previous_layer": [
            "Layer_45_Conv"
        ],
        "next_layer": [
            "Layer_47_Add"
        ]
    },
    "Layer_47_Add": {
        "name": "Layer_47_Add",
        "operation": "add",
        "size": 16384,
        "dtype": "int8",
        "pl_shift_bit": 6,
        "add_shift_bit": 6,
        "output_shift_bit": -1,
        "previous_layer": [
            "Layer_46_BatchNormalization",
            "Layer_41_Relu"
        ],
        "pl_name": "Layer_46_BatchNormalization",
        "add_name": "Layer_41_Relu",
        "next_layer": [
            "Layer_48_Relu"
        ]
    },
    "Layer_48_Relu": {
        "name": "Layer_48_Relu",
        "operation": "relu",
        "input_channel_num": 256,
        "input_size": {
            "height": 8,
            "width": 8
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "previous_layer": [
            "Layer_47_Add"
        ],
        "next_layer": [
            "Layer_49_Conv",
            "Layer_54_Conv"
        ]
    },
    "Layer_49_Conv": {
        "name": "Layer_49_Conv",
        "operation": "conv",
        "output_shift": 8,
        "bias_shift": 14,
        "load_bias": false,
        "input_channel_num": 256,
        "output_channel_num": 512,
        "input_size": {
            "height": 8,
            "width": 8
        },
        "padding": {
            "top": 1,
            "bottom": 1,
            "left": 1,
            "right": 1
        },
        "stride": {
            "height": 2,
            "width": 2
        },
        "kernel_size": {
            "height": 3,
            "width": 3
        },
        "output_size": {
            "height": 4,
            "width": 4
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "weight_dtype": "int8",
        "bias_dtype": "float",
        "previous_layer": [
            "Layer_48_Relu"
        ],
        "next_layer": [
            "Layer_50_BatchNormalization"
        ]
    },
    "Layer_50_BatchNormalization": {
        "name": "Layer_50_BatchNormalization",
        "operation": "batchnorm",
        "input_size": {
            "height": 4,
            "width": 4
        },
        "input_channel_num": 512,
        "input_dtype": "int8",
        "input_div_shift": 6,
        "mean_dtype": "float",
        "beta_dtype": "float",
        "var_dtype": "float",
        "scale_dtype": "float",
        "output_dtype": "int8",
        "output_scale": 7,
        "epsilon": 9.999999747378752e-06,
        "previous_layer": [
            "Layer_49_Conv"
        ],
        "next_layer": [
            "Layer_51_Relu"
        ]
    },
    "Layer_51_Relu": {
        "name": "Layer_51_Relu",
        "operation": "relu",
        "input_channel_num": 512,
        "input_size": {
            "height": 4,
            "width": 4
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "previous_layer": [
            "Layer_50_BatchNormalization"
        ],
        "next_layer": [
            "Layer_52_Conv"
        ]
    },
    "Layer_52_Conv": {
        "name": "Layer_52_Conv",
        "operation": "conv",
        "output_shift": 11,
        "bias_shift": 17,
        "load_bias": false,
        "input_channel_num": 512,
        "output_channel_num": 512,
        "input_size": {
            "height": 4,
            "width": 4
        },
        "padding": {
            "top": 1,
            "bottom": 1,
            "left": 1,
            "right": 1
        },
        "stride": {
            "height": 1,
            "width": 1
        },
        "kernel_size": {
            "height": 3,
            "width": 3
        },
        "output_size": {
            "height": 4,
            "width": 4
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "weight_dtype": "int8",
        "bias_dtype": "float",
        "previous_layer": [
            "Layer_51_Relu"
        ],
        "next_layer": [
            "Layer_53_BatchNormalization"
        ]
    },
    "Layer_53_BatchNormalization": {
        "name": "Layer_53_BatchNormalization",
        "operation": "batchnorm",
        "input_size": {
            "height": 4,
            "width": 4
        },
        "input_channel_num": 512,
        "input_dtype": "int8",
        "input_div_shift": 6,
        "mean_dtype": "float",
        "beta_dtype": "float",
        "var_dtype": "float",
        "scale_dtype": "float",
        "output_dtype": "int8",
        "output_scale": 6,
        "epsilon": 9.999999747378752e-06,
        "previous_layer": [
            "Layer_52_Conv"
        ],
        "next_layer": [
            "Layer_56_Add"
        ]
    },
    "Layer_54_Conv": {
        "name": "Layer_54_Conv",
        "operation": "conv",
        "output_shift": 6,
        "bias_shift": 14,
        "load_bias": false,
        "input_channel_num": 256,
        "output_channel_num": 512,
        "input_size": {
            "height": 8,
            "width": 8
        },
        "padding": {
            "top": 0,
            "bottom": 0,
            "left": 0,
            "right": 0
        },
        "stride": {
            "height": 2,
            "width": 2
        },
        "kernel_size": {
            "height": 1,
            "width": 1
        },
        "output_size": {
            "height": 4,
            "width": 4
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "weight_dtype": "int8",
        "bias_dtype": "float",
        "previous_layer": [
            "Layer_48_Relu"
        ],
        "next_layer": [
            "Layer_55_BatchNormalization"
        ]
    },
    "Layer_55_BatchNormalization": {
        "name": "Layer_55_BatchNormalization",
        "operation": "batchnorm",
        "input_size": {
            "height": 4,
            "width": 4
        },
        "input_channel_num": 512,
        "input_dtype": "int8",
        "input_div_shift": 8,
        "mean_dtype": "float",
        "beta_dtype": "float",
        "var_dtype": "float",
        "scale_dtype": "float",
        "output_dtype": "int8",
        "output_scale": 6,
        "epsilon": 9.999999747378752e-06,
        "previous_layer": [
            "Layer_54_Conv"
        ],
        "next_layer": [
            "Layer_56_Add"
        ]
    },
    "Layer_56_Add": {
        "name": "Layer_56_Add",
        "operation": "add",
        "size": 8192,
        "dtype": "int8",
        "pl_shift_bit": 6,
        "add_shift_bit": 6,
        "output_shift_bit": -1,
        "previous_layer": [
            "Layer_53_BatchNormalization",
            "Layer_55_BatchNormalization"
        ],
        "pl_name": "Layer_53_BatchNormalization",
        "add_name": "Layer_55_BatchNormalization",
        "next_layer": [
            "Layer_57_Relu"
        ]
    },
    "Layer_57_Relu": {
        "name": "Layer_57_Relu",
        "operation": "relu",
        "input_channel_num": 512,
        "input_size": {
            "height": 4,
            "width": 4
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "previous_layer": [
            "Layer_56_Add"
        ],
        "next_layer": [
            "Layer_58_Conv",
            "Layer_63_Add"
        ]
    },
    "Layer_58_Conv": {
        "name": "Layer_58_Conv",
        "operation": "conv",
        "output_shift": 10,
        "bias_shift": 15,
        "load_bias": false,
        "input_channel_num": 512,
        "output_channel_num": 512,
        "input_size": {
            "height": 4,
            "width": 4
        },
        "padding": {
            "top": 1,
            "bottom": 1,
            "left": 1,
            "right": 1
        },
        "stride": {
            "height": 1,
            "width": 1
        },
        "kernel_size": {
            "height": 3,
            "width": 3
        },
        "output_size": {
            "height": 4,
            "width": 4
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "weight_dtype": "int8",
        "bias_dtype": "float",
        "previous_layer": [
            "Layer_57_Relu"
        ],
        "next_layer": [
            "Layer_59_BatchNormalization"
        ]
    },
    "Layer_59_BatchNormalization": {
        "name": "Layer_59_BatchNormalization",
        "operation": "batchnorm",
        "input_size": {
            "height": 4,
            "width": 4
        },
        "input_channel_num": 512,
        "input_dtype": "int8",
        "input_div_shift": 5,
        "mean_dtype": "float",
        "beta_dtype": "float",
        "var_dtype": "float",
        "scale_dtype": "float",
        "output_dtype": "int8",
        "output_scale": 7,
        "epsilon": 9.999999747378752e-06,
        "previous_layer": [
            "Layer_58_Conv"
        ],
        "next_layer": [
            "Layer_60_Relu"
        ]
    },
    "Layer_60_Relu": {
        "name": "Layer_60_Relu",
        "operation": "relu",
        "input_channel_num": 512,
        "input_size": {
            "height": 4,
            "width": 4
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "previous_layer": [
            "Layer_59_BatchNormalization"
        ],
        "next_layer": [
            "Layer_61_Conv"
        ]
    },
    "Layer_61_Conv": {
        "name": "Layer_61_Conv",
        "operation": "conv",
        "output_shift": 11,
        "bias_shift": 18,
        "load_bias": false,
        "input_channel_num": 512,
        "output_channel_num": 512,
        "input_size": {
            "height": 4,
            "width": 4
        },
        "padding": {
            "top": 1,
            "bottom": 1,
            "left": 1,
            "right": 1
        },
        "stride": {
            "height": 1,
            "width": 1
        },
        "kernel_size": {
            "height": 3,
            "width": 3
        },
        "output_size": {
            "height": 4,
            "width": 4
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "weight_dtype": "int8",
        "bias_dtype": "float",
        "previous_layer": [
            "Layer_60_Relu"
        ],
        "next_layer": [
            "Layer_62_BatchNormalization"
        ]
    },
    "Layer_62_BatchNormalization": {
        "name": "Layer_62_BatchNormalization",
        "operation": "batchnorm",
        "input_size": {
            "height": 4,
            "width": 4
        },
        "input_channel_num": 512,
        "input_dtype": "int8",
        "input_div_shift": 7,
        "mean_dtype": "float",
        "beta_dtype": "float",
        "var_dtype": "float",
        "scale_dtype": "float",
        "output_dtype": "int8",
        "output_scale": 6,
        "epsilon": 9.999999747378752e-06,
        "previous_layer": [
            "Layer_61_Conv"
        ],
        "next_layer": [
            "Layer_63_Add"
        ]
    },
    "Layer_63_Add": {
        "name": "Layer_63_Add",
        "operation": "add",
        "size": 8192,
        "dtype": "int8",
        "pl_shift_bit": 6,
        "add_shift_bit": 5,
        "output_shift_bit": 0,
        "previous_layer": [
            "Layer_62_BatchNormalization",
            "Layer_57_Relu"
        ],
        "pl_name": "Layer_62_BatchNormalization",
        "add_name": "Layer_57_Relu",
        "next_layer": [
            "Layer_64_Relu"
        ]
    },
    "Layer_64_Relu": {
        "name": "Layer_64_Relu",
        "operation": "relu",
        "input_channel_num": 512,
        "input_size": {
            "height": 4,
            "width": 4
        },
        "input_dtype": "int8",
        "output_dtype": "int8",
        "previous_layer": [
            "Layer_63_Add"
        ],
        "next_layer": [
            "Layer_65_Pad"
        ]
    },
    "Layer_65_Pad": {
        "name": "Layer_65_Pad",
        "operation": "pad",
        "previous_layer": [
            "Layer_64_Relu"
        ],
        "next_layer": [
            "Layer_66_AveragePool"
        ]
    },
    "Layer_66_AveragePool": {
        "name": "Layer_66_AveragePool",
        "operation": "pooling",
        "input_dtype": "int8",
        "output_dtype": "int8",
        "pool_type": "avg",
        "input_channel_num": 512,
        "output_channel_num": 512,
        "input_size": {
            "height": 4,
            "width": 4
        },
        "padding": {
            "top": 0,
            "bottom": 0,
            "left": 0,
            "right": 0
        },
        "stride": {
            "height": 4,
            "width": 4
        },
        "output_size": {
            "height": 1,
            "width": 1
        },
        "kernel_size": {
            "height": 4,
            "width": 4
        },
        "previous_layer": [
            "Layer_65_Pad"
        ],
        "next_layer": [
            "Layer_67_Reshape"
        ]
    },
    "Layer_67_Reshape": {
        "name": "Layer_67_Reshape",
        "operation": "reshape",
        "previous_layer": [
            "Layer_66_AveragePool"
        ],
        "next_layer": [
            "Layer_68_Gemm"
        ]
    },
    "Layer_68_Gemm": {
        "name": "Layer_68_Gemm",
        "operation": "fc",
        "input_size": 512,
        "output_size": 10,
        "input_div_shift": 7,
        "input_dtype": "int8",
        "output_dtype": "int8",
        "output_shift": 10,
        "bias_shift": 6,
        "weight_dtype": "int8",
        "bias_dtype": "int8",
        "previous_layer": [
            "Layer_67_Reshape"
        ]
    }
}